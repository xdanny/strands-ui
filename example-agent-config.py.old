#!/usr/bin/env -S uv run
# /// script
# dependencies = [
#   "strands-agents-builder",
#   "openai",
# ]
# ///
"""
Configuration examples for different local model setups

This file shows how to configure Strands agents for various local
OpenAI-compatible endpoints.
"""

from strands import Agent
from strands.models.openai import OpenAIModel


# ============================================================================
# CONFIGURATION 1: LM Studio (Default)
# ============================================================================

def create_lmstudio_agent():
    """Agent configured for LM Studio"""
    model = OpenAIModel(
        client_args={
            "base_url": "http://localhost:1234/v1",
            "api_key": "lm-studio",
        },
        model_id="openai/gpt-oss-20b",  # Or whatever model you loaded in LM Studio
        params={
            "temperature": 0.7,
        }
    )
    return Agent(model=model)


# ============================================================================
# CONFIGURATION 2: Ollama
# ============================================================================

def create_ollama_agent():
    """Agent configured for Ollama"""
    model = OpenAIModel(
        client_args={
            "base_url": "http://localhost:11434/v1",
            "api_key": "ollama",
        },
        model_id="openai/gpt-oss-20b",  # Or: llama3, mistral, phi3, etc.
        params={
            "temperature": 0.7,
        }
    )
    return Agent(model=model)


# ============================================================================
# CONFIGURATION 3: Custom OpenAI-compatible endpoint
# ============================================================================

def create_custom_agent():
    """Agent configured for custom endpoint"""
    model = OpenAIModel(
        client_args={
            "base_url": "http://your-server:port/v1",
            "api_key": "your-api-key-if-needed",
        },
        model_id="openai/gpt-oss-20b",  # Change to your model
        params={
            "temperature": 0.7,
            "max_tokens": 2000,
        }
    )
    return Agent(model=model)


# ============================================================================
# CONFIGURATION 4: With tools/functions
# ============================================================================

def create_agent_with_tools():
    """Agent with custom tools"""
    from strands import tool

    @tool
    def calculate(expression: str) -> str:
        """
        Evaluate a mathematical expression.

        Args:
            expression: A mathematical expression to evaluate (e.g., "2 + 2", "10 * 5")
        """
        try:
            result = eval(expression)
            return f"The result is: {result}"
        except Exception as e:
            return f"Error evaluating expression: {e}"

    @tool
    def get_time() -> str:
        """Get the current time"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    model = OpenAIModel(
        client_args={
            "base_url": "http://localhost:1234/v1",
            "api_key": "not-needed",
        },
        model_id="openai/gpt-oss-20b",
        params={
            "temperature": 0.7,
        }
    )

    return Agent(
        model=model,
        tools=[calculate, get_time],
        system_prompt="You are a helpful assistant with access to tools. Use them when appropriate."
    )


# ============================================================================
# CONFIGURATION 5: With streaming
# ============================================================================

def create_streaming_agent():
    """Agent configured for streaming responses"""
    model = OpenAIModel(
        client_args={
            "base_url": "http://localhost:1234/v1",
            "api_key": "not-needed",
        },
        model_id="openai/gpt-oss-20b",
        params={
            "temperature": 0.7,
        }
    )
    return Agent(model=model)


def stream_response(agent, query):
    """Stream response from agent"""
    print(f"Query: {query}\n")
    print("Response: ", end="", flush=True)

    for event in agent.stream(query):
        if "data" in event:
            print(event["data"], end="", flush=True)

    print("\n")


# ============================================================================
# Example usage
# ============================================================================

if __name__ == "__main__":
    print("Configuration Examples for Strands + Local Models\n")
    print("=" * 60)

    # Example 1: Basic LM Studio setup
    print("\n1. Creating LM Studio agent...")
    agent1 = create_lmstudio_agent()
    print("✅ LM Studio agent created")

    # Example 2: Ollama setup
    print("\n2. Creating Ollama agent...")
    agent2 = create_ollama_agent()
    print("✅ Ollama agent created")

    # Example 3: With tools
    print("\n3. Creating agent with tools...")
    agent3 = create_agent_with_tools()
    print("✅ Agent with tools created")

    # Example 4: Test a query
    print("\n4. Testing a simple query...")
    try:
        response = agent1("Say hello in 5 words or less")
        print(f"Response: {response}")
    except Exception as e:
        print(f"⚠️  Error: {e}")
        print("Make sure your local server is running!")

    # Example 5: Streaming
    print("\n5. Testing streaming response...")
    try:
        agent4 = create_streaming_agent()
        stream_response(agent4, "Count from 1 to 5")
    except Exception as e:
        print(f"⚠️  Error: {e}")

    print("\n" + "=" * 60)
    print("\nTo use these agents:")
    print("1. Import the function you need")
    print("2. Call it to create an agent")
    print("3. Use agent(query) to get responses")
    print("\nExample:")
    print("  from example_agent_config import create_lmstudio_agent")
    print("  agent = create_lmstudio_agent()")
    print("  response = agent('Hello!')")
